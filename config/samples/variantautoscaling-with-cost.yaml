# Example: VariantAutoscaling with variantCost configuration
# This demonstrates how to configure different costs for variants to enable
# cost-based optimization in the capacity analyzer.
#
# Scenario: Two variants of the same model on different accelerators
# - Premium variant on H100 (higher cost, faster performance)
# - Standard variant on A100 (lower cost, standard performance)
#
# The capacity analyzer will prefer scaling the lower-cost variant when both
# can handle the load, enabling cost optimization.
#
# Note: Performance parameters (alpha, beta, gamma, delta) are no longer part
# of the CRD. WVA now uses actual runtime metrics from Prometheus for making
# scaling decisions based on saturation analysis.

---
apiVersion: llmd.ai/v1alpha1
kind: VariantAutoscaling
metadata:
  name: llama-70b-premium-h100
  namespace: llm-inference
  labels:
    app: llm-inference
    model: llama-70b
    variant: premium
    accelerator: h100
spec:
  # Reference to the target Deployment to scale (follows HPA pattern)
  scaleTargetRef:
    kind: Deployment
    name: llama-70b-premium-h100

  # Model identifier (OpenAI API compatible)
  modelID: "meta/llama-3.1-70b"

  # Premium tier: H100 accelerator with higher cost
  variantCost: "80.0"  # Cost per replica (reflects H100 premium pricing)

  # Model profile for H100 accelerator
  modelProfile:
    accelerators:
      - acc: "H100"
        accCount: 1
        maxBatchSize: 256

---
apiVersion: llmd.ai/v1alpha1
kind: VariantAutoscaling
metadata:
  name: llama-70b-standard-a100
  namespace: llm-inference
  labels:
    app: llm-inference
    model: llama-70b
    variant: standard
    accelerator: a100
spec:
  # Reference to the target Deployment to scale (follows HPA pattern)
  scaleTargetRef:
    kind: Deployment
    name: llama-70b-standard-a100

  # Model identifier (same model as premium variant)
  modelID: "meta/llama-3.1-70b"

  # Standard tier: A100 accelerator with standard cost
  variantCost: "40.0"  # Cost per replica (standard A100 pricing)

  # Model profile for A100 accelerator
  modelProfile:
    accelerators:
      - acc: "A100"
        accCount: 1
        maxBatchSize: 256

---
# Example: Budget variant with lower cost for best-effort workloads
apiVersion: llmd.ai/v1alpha1
kind: VariantAutoscaling
metadata:
  name: llama-8b-budget-t4
  namespace: llm-inference
  labels:
    app: llm-inference
    model: llama-8b
    variant: budget
    accelerator: t4
spec:
  # Reference to the target Deployment to scale (follows HPA pattern)
  scaleTargetRef:
    kind: Deployment
    name: llama-8b-budget-t4

  modelID: "meta/llama-3.1-8b"

  # Budget tier: Very low cost for cost-sensitive workloads
  variantCost: "5.0"  # Minimal cost for T4 GPUs

  modelProfile:
    accelerators:
      - acc: "T4"
        accCount: 1
        maxBatchSize: 64  # Smaller batch size for T4

---
# Example: Multi-tenant cost tracking
# Assign different costs to the same accelerator for different tenants
apiVersion: llmd.ai/v1alpha1
kind: VariantAutoscaling
metadata:
  name: llama-8b-tenant-gold
  namespace: tenant-gold
  labels:
    tenant: gold
    model: llama-8b
spec:
  # Reference to the target Deployment to scale (follows HPA pattern)
  scaleTargetRef:
    kind: Deployment
    name: llama-8b-tenant-gold

  modelID: "meta/llama-3.1-8b"

  # Gold tenant: Premium pricing for priority service
  variantCost: "25.0"  # Higher cost reflects priority/SLA

  modelProfile:
    accelerators:
      - acc: "A100"
        accCount: 1
        maxBatchSize: 256

---
apiVersion: llmd.ai/v1alpha1
kind: VariantAutoscaling
metadata:
  name: llama-8b-tenant-silver
  namespace: tenant-silver
  labels:
    tenant: silver
    model: llama-8b
spec:
  # Reference to the target Deployment to scale (follows HPA pattern)
  scaleTargetRef:
    kind: Deployment
    name: llama-8b-tenant-silver

  modelID: "meta/llama-3.1-8b"

  # Silver tenant: Standard pricing
  variantCost: "15.0"  # Standard cost

  modelProfile:
    accelerators:
      - acc: "A100"
        accCount: 1
        maxBatchSize: 256

---
# Note: If variantCost is not specified, it defaults to 10.0
# Example of default behavior:
apiVersion: llmd.ai/v1alpha1
kind: VariantAutoscaling
metadata:
  name: llama-8b-default-cost
  namespace: llm-inference
spec:
  # Reference to the target Deployment to scale (follows HPA pattern)
  scaleTargetRef:
    kind: Deployment
    name: llama-8b-default-cost

  modelID: "meta/llama-3.1-8b"

  # variantCost omitted - will default to 10.0

  modelProfile:
    accelerators:
      - acc: "A100"
        accCount: 1
        maxBatchSize: 256
